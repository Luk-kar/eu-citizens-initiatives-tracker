# ECI Follow-up Website Extractor

Processes **dedicated follow-up websites** (e.g., specific project pages linked from the main Commission response). It applies the same legislative analysis as the main response extractor but adapts it to the diverse and often nested HTML structures of these external or sub-sites.

## üöÄ Purpose

Dedicated follow-up pages often contain the *follow-up* implementation details that are missing from the initial Commission communication. This extractor:
- **Digs Deeper**: parses content from nested `<div>` structures and custom layouts (e.g., "Response of the Commission", "Next steps").
- **Verifies Implementation**: Extracts concrete dates and evidence of action (e.g., "law_implementation_date", "followup_events_with_dates").
- **Tracks Progress**: Identifies specific follow-up activities like workshops, partnerships, and roadmaps that might only be detailed on these dedicated pages.
- **Unifies Data**: Merges this granular data with the core initiative metadata (title, registration number) to provide key values to join later with the previous high-level response CSV.

## üìÇ Project Structure

- **`processor.py`**: Main orchestrator. It loads the *previous* step's CSV (`eci_responses_*.csv`) to validate registration numbers and get metadata, then processes all corresponding HTML files.
- **`model.py`**: Defines the `ECIFollowupWebsiteRecord` schema, which mirrors the response record but focuses on content found on the dedicated site.
- **`parser/extractors/`**:
  - **`main.py`**: The central `FollowupWebsiteExtractor` class that initializes sub-extractors.
  - **`outcome.py`**: Specialized logic (`FollowupWebsiteLegislativeOutcomeExtractor`) to find legislative status in non-standard page sections.
  - **`followup.py`**: Extracts events and dates specifically from the "Response of the Commission" section.
- **`__main__.py`**: Entry point for execution.

## ‚öôÔ∏è Workflow

1. **Discovery**:
   - Finds the latest timestamped directory in `data/`.
   - Loads the *existing* `eci_responses_*.csv` (generated by the `extractor.responses` module) to build a lookup of valid initiatives and their titles/URLs.

2. **HTML Processing**:
   - Iterates through HTML files in `responses_followup_website/{year}/`.
   - Validates that each file corresponds to a known initiative in the responses CSV.

3. **Extraction**:
   - **Lazy Loading**: Initializes specialized extractors (Outcome, Activity, Structural) only when needed.
   - **Logic Reuse**: Inherits and adapts the core legislative analysis logic (keywords, patterns, status hierarchy) directly from the `responses` module to ensure consistent classification.
   - **Section Targeting**: Locates specialized headers like "Response of the Commission" or "Follow-up on the Commission's actions" to isolate relevant text in non-standard layouts.
   - **Text Analysis**: Extracts deadlines, promises ("commission_promised_new_law"), and actions ("laws_actions", "policies_actions").

4. **Output**: Generates a new CSV `eci_responses_followup_website_{TIMESTAMP}.csv` in the same run directory.

## üìä Data Fields

The output CSV (`eci_responses_followup_website_{TIMESTAMP}.csv`) contains detailed tracking data:

| Category | Key Fields |
| :--- | :--- |
| **Outcome** | `final_outcome_status`, `law_implementation_date` |
| **Actions** | `laws_actions` (JSON), `policies_actions` (JSON), `followup_events_with_dates` (JSON) |
| **Commitments** | `commission_promised_new_law` (Bool), `commission_deadlines` (JSON) |
| **Follow-up** | `has_roadmap`, `has_workshop`, `has_partnership_programs`, `followup_latest_date`, `followup_most_future_date` |
| **References** | `official_communication_document_urls`, `referenced_legislation_by_id`, `court_cases_referenced` |
| **Metadata** | `registration_number`, `initiative_title`, `followup_dedicated_website` |

### Python Dependencies

The extractor relies on `BeautifulSoup` for parsing. Install the required libraries using the production requirements file:

```bash
pip install -r ECI_initiatives/requirements.prod.txt
```

## üñ•Ô∏è Usage

Run the module from the project root. It relies on the data generated by previous pipeline steps.

```bash
# Using uv (recommended)
uv run -m extractor.responses_followup_website

# Standard python
python -m extractor.responses_followup_website
```

**Output Location:**  
`data/{TIMESTAMP}/eci_responses_followup_website_{DATE}.csv`

## üîó Dependencies

- **Critical Dependency**: Must be run *after* `extractor.responses`, as it reads the `eci_responses_*.csv` file to validate registration numbers and retrieve initiative titles.
- Depends on the `responses_followup_website/` HTML files created by `scraper.responses_followup_website`.
```