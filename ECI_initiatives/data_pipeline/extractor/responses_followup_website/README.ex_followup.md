# ECI Follow-up Website Extractor

Processes **dedicated follow-up websites** (e.g., specific project pages linked from the main Commission response). It applies the same legislative analysis as the main response extractor but adapts it to the diverse and often nested HTML structures of these external or sub-sites.

## ðŸš€ Purpose

Dedicated follow-up pages often contain the *follow-up* implementation details that are missing from the initial Commission communication. This extractor:
- **Digs Deeper**: parses content from nested `<div>` structures and custom layouts (e.g., "Response of the Commission", "Next steps").
- **Verifies Implementation**: Extracts concrete dates and evidence of action (e.g., "law_implementation_date", "followup_events_with_dates").
- **Tracks Progress**: Identifies specific follow-up activities like workshops, partnerships, and roadmaps that might only be detailed on these dedicated pages.
- **Unifies Data**: Merges this granular data with the core initiative metadata (title, registration number) to provide key values to join later with the previous high-level response CSV.

## ðŸ“‚ Project Structure

- **`processor.py`**: Main orchestrator. It loads the *previous* step's CSV (`eci_responses_*.csv`) to validate registration numbers and get metadata, then processes all corresponding HTML files.
- **`model.py`**: Defines the `ECIFollowupWebsiteRecord` schema, which mirrors the response record but focuses on content found on the dedicated site.
- **`parser/extractors/`**:
  - **`main.py`**: The central `FollowupWebsiteExtractor` class that initializes sub-extractors.
  - **`outcome.py`**: Specialized logic (`FollowupWebsiteLegislativeOutcomeExtractor`) to find legislative status in non-standard page sections.
  - **`followup.py`**: Extracts events and dates specifically from the "Response of the Commission" section.
- **`__main__.py`**: Entry point for execution.
- **`tests/`**: Comprehensive test suite for extraction logic and data processing.

## âš™ï¸ Workflow

1. **Discovery**:
   - Finds the latest timestamped directory in `data/`.
   - Loads the *existing* `eci_responses_*.csv` (generated by the `extractor.responses` module) to build a lookup of valid initiatives and their titles/URLs.

2. **HTML Processing**:
   - Iterates through HTML files in `responses_followup_website/{year}/`.
   - Validates that each file corresponds to a known initiative in the responses CSV.

3. **Extraction**:
   - **Lazy Loading**: Initializes specialized extractors (Outcome, Activity, Structural) only when needed.
   - **Logic Reuse**: Inherits and adapts the core legislative analysis logic (keywords, patterns, status hierarchy) directly from the `responses` module to ensure consistent classification.
   - **Section Targeting**: Locates specialized headers like "Response of the Commission" or "Follow-up on the Commission's actions" to isolate relevant text in non-standard layouts.
   - **Text Analysis**: Extracts deadlines, promises ("commission_promised_new_law"), and actions ("laws_actions", "policies_actions").

4. **Output**: Generates a new CSV `eci_responses_followup_website_{TIMESTAMP}.csv` in the same run directory.

## ðŸ“Š Data Fields

The output CSV (`eci_responses_followup_website_{TIMESTAMP}.csv`) contains detailed tracking data:

| Category | Key Fields |
| :--- | :--- |
| **Outcome** | `final_outcome_status`, `law_implementation_date` |
| **Actions** | `laws_actions` (JSON), `policies_actions` (JSON), `followup_events_with_dates` (JSON) |
| **Commitments** | `commission_promised_new_law` (Bool), `commission_deadlines` (JSON) |
| **Follow-up** | `has_roadmap`, `has_workshop`, `has_partnership_programs`, `followup_latest_date`, `followup_most_future_date` |
| **References** | `official_communication_document_urls`, `referenced_legislation_by_id`, `court_cases_referenced` |
| **Metadata** | `registration_number`, `initiative_title`, `followup_dedicated_website` |

## ðŸ› ï¸ Prerequisites

Ensure you have the following installed:
- **Python 3.8+**

### Python Dependencies

The extractor relies on `BeautifulSoup` for parsing. Install the required libraries using the production requirements file:

```bash
pip install -r ECI_initiatives/data_pipeline/requirements.prod.txt
```

> [!IMPORTANT]
> **Data Dependency**: This module requires data from previous pipeline steps (`extractor.responses` and `scraper.responses_followup_website`).

## ðŸ–¥ï¸ Usage

It is recommended to use **[uv](https://github.com/astral-sh/uv)** for fast dependency management and execution.

Navigate to the `ECI_initiatives` directory:
```bash
cd ECI_initiatives
```

### 1. Setup Environment
Create the virtual environment and install dependencies:
```bash
uv venv
uv pip install -r requirements.prod.txt
```

### 2. Activate Shell
Load the virtual environment into your current shell session:

**Linux/macOS:**
```bash
source .venv/bin/activate
```

**Windows:**
```powershell
.venv\Scripts\activate
```

### 3. Run the Extractor
Once the environment is active (you should see `(.venv)` in your prompt), run the module. It will automatically find the latest data directory:

```bash
python -m data_pipeline.extractor.responses_followup_website
```

**Output Location:**  
`data/{TIMESTAMP}/eci_responses_followup_website_{DATE}.csv`

## ðŸ§ª Testing

The extractor includes a comprehensive test suite located in the `tests/` directory. Tests are organized by functionality to ensure data extraction accuracy, legislative outcome classification, and system reliability.

### Using the Test Runner Script

From the project root, use the `run_tests.py` script:

```bash
# Run all followup website extractor tests
python run_tests.py --extractor --followup-website

# Run only behaviour tests
python run_tests.py --extractor --followup-website --behaviour

# Run with verbose output and coverage
python run_tests.py --extractor --followup-website --coverage
```

### Test Requirements

Tests require additional dependencies beyond production requirements. Install development dependencies:

```bash
pip install -r ECI_initiatives/tests/requirements.test.txt
```

> [!IMPORTANT]
> Tests use example HTML files located in `ECI_initiatives/tests/data/example_htmls/responses_followup_website/` for validation. These fixtures represent real-world HTML structures from dedicated ECI follow-up websites.

## ðŸ”— Dependencies

- **Critical Dependency**: Must be run *after* `extractor.responses`, as it reads the `eci_responses_*.csv` file to validate registration numbers and retrieve initiative titles.
- Depends on the `responses_followup_website/` HTML files created by `scraper.responses_followup_website`.
