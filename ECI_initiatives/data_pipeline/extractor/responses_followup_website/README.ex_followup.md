# ECI Follow-up Website Scraper

Scrapes **dedicated follow-up websites** referenced in the Commission's responses to European Citizens' Initiatives. It reads the `responses_list.csv` file generated by the previous step, extracts URLs from the `followup_dedicated_website` column, and downloads those pages.

> [!IMPORTANT]
> **Dependency Chain**: This scraper requires the complete pipeline:
> 
> `initiatives` scraper â†’ `responses` scraper â†’ **this scraper**
> 
> You must run both previous scrapers first

## ğŸ“‚ Project Structure

- **`processor.py`**: Main orchestrator. It loads the *previous* step's CSV (`eci_responses_*.csv`) to validate registration numbers and get metadata, then processes all corresponding HTML files.
- **`model.py`**: Defines the `ECIFollowupWebsiteRecord` schema, which mirrors the response record but focuses on content found on the dedicated site.
- **`parser/extractors/`**:
  - **`main.py`**: The central `FollowupWebsiteExtractor` class that initializes sub-extractors.
  - **`outcome.py`**: Specialized logic (`FollowupWebsiteLegislativeOutcomeExtractor`) to find legislative status in non-standard page sections.
  - **`followup.py`**: Extracts events and dates specifically from the "Response of the Commission" section.
- **`__main__.py`**: Entry point for execution.
- **`tests/`**: Comprehensive test suite for extraction logic and data processing.

## ğŸš€ Flow

1. **Prerequisite Check**:
   - Finds the **latest timestamped run directory** in `data/`.
   - Locates the most recent `eci_responses_*.csv` file (generated by `responses` scraper).

2. **URL Extraction**:
   - Reads the CSV and filters rows where `followup_dedicated_website` contains a URL.
   - Extracts `registration_number` and parses the `year` for file organization.
   - Normalizes registration numbers (e.g., `YYYY/NNNNNN` â†’ `YYYY_NNNNNN` for filenames).

3. **Follow-up Download Loop**:
   - Initializes a headless Chrome browser.
   - Visits each extracted URL with randomized delays and retry logic.
   - **Saves** HTML to `responses_followup_website/{year}/{reg_number}_en.html`.

4. **Resilience & Recovery**:
   - Detects rate limits ("429") and applies exponential backoff.
   - Validates HTML content to reject error pages or empty responses.

5. **Finalization**:
   - Logs comprehensive summary (URLs found, downloads successful/failed).
   - No new CSV generated (uses existing `eci_responses_*.csv` as source).

## ğŸ› ï¸ Prerequisites


Ensure you have the following installed:
- **Python 3.8+**
- **Google Chrome** (latest version)
- **ChromeDriver** (matching your Chrome version)

### Python Dependencies

See the [main project documentation](../../README.ECI_initiatives.md#-quick-start-end-to-end) for detailed installation instructions.

```bash
pip install -r ECI_initiatives/data_pipeline/requirements.prod.txt
```
## âš™ï¸ Configuration

Key settings can be modified in `consts.py`:

| Constant | Description | Default |
| :--- | :--- | :--- |
| `WAIT_BETWEEN_DOWNLOADS` | Delay between requests | `1.5 - 1.9s` |
| `RETRY_WAIT_BASE` | Base time for exponential backoff | `2.0 - 2.5s` |
| `FOLLOWUP_WEBSITE_FILENAME_PATTERN` | Naming convention | `{year}/{registration_number}_en.html` |

## ğŸ“¦ Output Structure

This scraper **appends** to the existing run directory:

```text
data/
â””â”€â”€ YYYY-MM-DD_HH-MM-SS/         # Existing timestamped folder
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ scraper_responses_followup_website_....log  # New log file
    â”œâ”€â”€ responses/               # (Read-only) Input files
    â””â”€â”€ responses_followup_website/  # NEW: Downloaded follow-up HTMLs
        â”œâ”€â”€ 2019/
        â”‚   â””â”€â”€ 000007_en.html
        â””â”€â”€ ...
```

## ğŸ–¥ï¸ Usage

For detailed setup and environment configuration, see the [main project documentation](../../README.ECI_initiatives.md#-quick-start-end-to-end).

**Quick Start:**

```bash
# From project root
cd ECI_initiatives
```
### 1. Setup Environment
Create the virtual environment and install dependencies:
```bash
uv venv
uv pip install -r data_pipeline/requirements.prod.txt
```

### 2. Activate Shell
Load the virtual environment into your current shell session:

**Linux/macOS:**
```bash
source .venv/bin/activate
```

**Windows:**
```powershell
.venv\Scripts\activate
```

### 3. Run the Scraper
Once the environment is active (you should see `(.venv)` in your prompt), run the module:

```bash
python -m data_pipeline.scraper.responses_followup_website
```