# ECI Commission Responses Scraper

Extension designed to scrape the **Commission's answer and follow-up** pages for European Citizens' Initiatives. Unlike the primary scraper, this tool **does not crawl listings from scratch**. Instead, it parses the local HTML files already downloaded by the `initiatives` scraper to discover response links, then fetches those specific documents.

> [!IMPORTANT]
> **Dependency Warning**: This scraper depends on the output of the `initiatives` scraper. You must run `scraper.initiatives` first to generate the necessary `data/YYYY-MM-DD.../initiatives` directory structure.

## ğŸ“‚ Project Structure

The module is organized as a Python package with the following key components:

- **`downloader.py`**: Handles the retrieval of response pages with retry logic and rate-limit handling.
- **`html_parser.py`**: Parses *local* initiative HTML files to extract links to Commission responses.
- **`file_operations/`**: Manages file system tasks (saving response HTMLs, updating CSVs).
- **`consts.py`**: Configuration for URLs, timeouts, file paths, and retry parameters.
- **`css_selectors.py`**: Centralized storage for response-page specific selectors.
- **`errors.py`**: Custom exceptions (e.g., `MissingDataDirectoryError`).

## ğŸš€ Flow

1. **Prerequisite Check**:
   - Scans the `data/` directory to find the **latest timestamped run** (generated by the `initiatives` scraper).
   - Verifies the existence of the `initiatives/` subdirectory containing raw HTML files.

2. **Link Extraction**:
   - Iterates through every local `{year}/{number}_en.html` file from the previous step.
   - Parses the HTML to find the specific anchor tag: *"Commission's answer and follow-up"*.
   - Compiles a list of target URLs to scrape.

3. **Response Download Loop**:
   - Initializes a headless Chrome browser.
   - Visits each extracted URL with a random delay strategy.
   - **Saves** the raw HTML of the response page into a new `responses/` subdirectory.

4. **Resilience & Recovery**:
   - Monitors for "429 - Too Many Requests" and automatically applies exponential backoff.
   - Logs failed URLs for manual review.

5. **Finalization**:
   - Generates (or updates) `responses_list.csv` with metadata (URL, registration number, title, download timestamp).
   - Normalizes registration numbers (e.g., converting `_` to `/`) to ensure easy joining with the main dataset.

## ğŸ› ï¸ Prerequisites

Ensure you have the following installed:
- **Python 3.8+**
- **Google Chrome** (latest version)
- **ChromeDriver** (matching your Chrome version)

### Python Dependencies

Install the required libraries using the production requirements file:

```bash
pip install -r ECI_initiatives/requirements.prod.txt
```

## âš™ï¸ Configuration

Key settings can be modified in `consts.py`:

| Constant | Description | Default |
| :--- | :--- | :--- |
| `WAIT_BETWEEN_DOWNLOADS` | Delay between requests | `1.5 - 1.9s` |
| `RETRY_WAIT_BASE` | Base time for exponential backoff | `2.0 - 2.5s` |
| `CSV_FILENAME` | Output filename | `responses_list.csv` |
| `RESPONSE_PAGE_FILENAME_PATTERN` | Naming convention | `{year}/{number}_en.html` |

## ğŸ“¦ Output Structure

This scraper **appends** to the existing run directory created by the `initiatives` scraper:

```text
data/
â””â”€â”€ YYYY-MM-DD_HH-MM-SS/         # Existing timestamped folder
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ scraper_responses_....log  # New log file
    â”œâ”€â”€ initiatives/             # (Read-only) Input files
    â”œâ”€â”€ responses/               # NEW: Downloaded response HTMLs
    â”‚   â”œâ”€â”€ 2019/
    â”‚   â”‚   â””â”€â”€ 2019_000007_en.html
    â”‚   â””â”€â”€ ...
    â””â”€â”€ responses_list.csv       # NEW: Structured response data
```

## ğŸ“Š Data Fields

**Purpose**: The `responses_list.csv` enables **response rate analysis** by cataloging initiatives that received Commission follow-up, with normalized keys for joining to the main dataset and tracking response coverage.

The generated `responses_list.csv` includes:
- `url_find_initiative`: The original URL where the link was found.
- `registration_number`: Unique ECI ID (normalized format `YYYY/NNNNNN`).
- `title`: Title of the initiative.
- `datetime`: Timestamp of the download.

## ğŸ–¥ï¸ Usage

It is recommended to use **[uv](https://github.com/astral-sh/uv)** for fast dependency management and execution.

Navigate to the `ECI_initiatives` directory:
```bash
cd ECI_initiatives
```

### 1. Setup Environment
Create the virtual environment and install dependencies:
```bash
uv venv
uv pip install -r requirements.prod.txt
```

### 2. Activate Shell
Load the virtual environment into your current shell session:

**Linux/macOS:**
```bash
source .venv/bin/activate
```

**Windows:**
```powershell
.venv\Scripts\activate
```

### 3. Run the Scraper
Once the environment is active (you should see `(.venv)` in your prompt), run the module:

```bash
python -m scraper.responses
```