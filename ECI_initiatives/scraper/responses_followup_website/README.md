# ECI Follow-up Website Scraper

Scrapes **dedicated follow-up websites** referenced in the Commission's responses to European Citizens' Initiatives. It reads the `responses_list.csv` file generated by the previous step, extracts URLs from the `followup_dedicated_website` column, and downloads those pages.

> [!IMPORTANT]
> **Dependency Chain**: This scraper requires the complete pipeline:
> 
> `initiatives` scraper â†’ `responses` scraper â†’ **this scraper**
> 
> You must run both previous scrapers first to generate the `eci_responses_*.csv` file.

## ğŸ“‚ Project Structure

The module is organized as a Python package with the following key components:

- **`downloader.py`**: Handles retrieval of follow-up website pages with retry logic.
- **`file_operations/csv_reader.py`**: Parses the `eci_responses_*.csv` to extract target URLs.
- **`file_operations/page.py`**: Saves HTML files with validation and error detection.
- **`consts.py`**: Configuration for timeouts, file paths, and retry parameters.
- **`errors.py`**: Custom exceptions (`MissingCSVFileError`, `MissingDataDirectoryError`).

## ğŸš€ Flow

1. **Prerequisite Check**:
   - Finds the **latest timestamped run directory** in `data/`.
   - Locates the most recent `eci_responses_*.csv` file (generated by `responses` scraper).

2. **URL Extraction**:
   - Reads the CSV and filters rows where `followup_dedicated_website` contains a URL.
   - Extracts `registration_number` and parses the `year` for file organization.
   - Normalizes registration numbers (e.g., `YYYY/NNNNNN` â†’ `YYYY_NNNNNN` for filenames).

3. **Follow-up Download Loop**:
   - Initializes a headless Chrome browser.
   - Visits each extracted URL with randomized delays and retry logic.
   - **Saves** HTML to `responses_followup_website/{year}/{reg_number}_en.html`.

4. **Resilience & Recovery**:
   - Detects rate limits ("429") and applies exponential backoff.
   - Validates HTML content to reject error pages or empty responses.

5. **Finalization**:
   - Logs comprehensive summary (URLs found, downloads successful/failed).
   - No new CSV generated (uses existing `eci_responses_*.csv` as source).

## ğŸ› ï¸ Prerequisites

Ensure you have the following installed:
- **Python 3.8+**
- **Google Chrome** (latest version)
- **ChromeDriver** (matching your Chrome version)

### Python Dependencies

Install the required libraries using the production requirements file:

```bash
pip install -r ECI_initiatives/requirements.prod.txt
```

## âš™ï¸ Configuration

Key settings can be modified in `consts.py`:

| Constant | Description | Default |
| :--- | :--- | :--- |
| `WAIT_BETWEEN_DOWNLOADS` | Delay between requests | `1.5 - 1.9s` |
| `RETRY_WAIT_BASE` | Base time for exponential backoff | `2.0 - 2.5s` |
| `FOLLOWUP_WEBSITE_FILENAME_PATTERN` | Naming convention | `{year}/{registration_number}_en.html` |

## ğŸ“¦ Output Structure

This scraper **appends** to the existing run directory:

```text
data/
â””â”€â”€ YYYY-MM-DD_HH-MM-SS/         # Existing timestamped folder
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ scraper_responses_followup_website_....log  # New log file
    â”œâ”€â”€ responses/               # (Read-only) Input files
    â””â”€â”€ responses_followup_website/  # NEW: Downloaded follow-up HTMLs
        â”œâ”€â”€ 2019/
        â”‚   â””â”€â”€ 000007_en.html
        â””â”€â”€ ...
```

## ğŸ–¥ï¸ Usage

It is recommended to use **[uv](https://github.com/astral-sh/uv)** for fast dependency management and execution.

Navigate to the `ECI_initiatives` directory:
```bash
cd ECI_initiatives
```

### 1. Setup Environment
Create the virtual environment and install dependencies:
```bash
uv venv
uv pip install -r requirements.prod.txt
```

### 2. Activate Shell
Load the virtual environment into your current shell session:

**Linux/macOS:**
```bash
source .venv/bin/activate
```

**Windows:**
```powershell
.venv\Scripts\activate
```

### 3. Run the Scraper
Once the environment is active (you should see `(.venv)` in your prompt), run the module:

```bash
python -m scraper.responses_followup_website
```
```